{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489263c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import PyPDF2\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import itertools\n",
    "\n",
    "# ---------- 0.1 IMPORTAR LIBRERÃAS PARA TKINTER----------\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da72bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6877d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b197d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f529cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(folder_path):\n",
    "    texts = []\n",
    "    pdf_files = glob.glob(os.path.join(folder_path, \"*.pdf\"))\n",
    "\n",
    "    log(f\"Se han encontrado {len(pdf_files)} PDF.\")\n",
    "    if len(pdf_files) == 0:\n",
    "        return []\n",
    "\n",
    "    for file in pdf_files:\n",
    "        try:\n",
    "            with open(file, \"rb\") as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                pages_text = []\n",
    "                for page in reader.pages:\n",
    "                    t = page.extract_text()\n",
    "                    if t:\n",
    "                        pages_text.append(t)\n",
    "                texts.append(\"\\n\".join(pages_text))\n",
    "                log(f\"  âœ” LeÃ­do: {os.path.basename(file)}\")\n",
    "        except Exception as e:\n",
    "            log(f\"  âŒ Error leyendo {file}: {e}\")\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e84593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.is_alpha\n",
    "        and token.lemma_ not in stop_words\n",
    "        and len(token.lemma_) > 2\n",
    "    ]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "064731a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurrence_matrix(texts, tf_df, top_n=50):\n",
    "    top_words = tf_df[\"word\"].head(top_n).tolist()\n",
    "    matrix = pd.DataFrame(0, index=top_words, columns=top_words)\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        unique_tokens = set(tokens)\n",
    "        filtered = [t for t in unique_tokens if t in top_words]\n",
    "\n",
    "        for w1, w2 in itertools.combinations(filtered, 2):\n",
    "            matrix.loc[w1, w2] += 1\n",
    "            matrix.loc[w2, w1] += 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b0cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TKINTER UI\n",
    "\n",
    "def log(msg):\n",
    "    text_log.insert(tk.END, msg + \"\\n\")\n",
    "    text_log.see(tk.END)\n",
    "\n",
    "\n",
    "def select_folder():\n",
    "    folder = filedialog.askdirectory()\n",
    "    if folder:\n",
    "        folder_path_var.set(folder)\n",
    "\n",
    "\n",
    "def run_processing():\n",
    "    folder = folder_path_var.get()\n",
    "    if not folder or not os.path.isdir(folder):\n",
    "        messagebox.showerror(\"Error\", \"Debes seleccionar una carpeta vÃ¡lida.\")\n",
    "        return\n",
    "\n",
    "    log(\"\\n==============================\")\n",
    "    log(\"   INICIANDO PROCESAMIENTO\")\n",
    "    log(\"==============================\\n\")\n",
    "\n",
    "    # 1. Cargar PDFs\n",
    "    log(\"ðŸ“„ Cargando PDFs...\")\n",
    "    raw_corpus = load_pdfs(folder)\n",
    "    if len(raw_corpus) == 0:\n",
    "        messagebox.showerror(\"Error\", \"No se encontraron PDFs.\")\n",
    "        return\n",
    "\n",
    "    # 2. Preprocesar\n",
    "    log(\"\\nðŸ”§ Preprocesando textos...\")\n",
    "    clean_corpus = [preprocess(t) for t in raw_corpus]\n",
    "\n",
    "    # 3. TF\n",
    "    if var_tf.get():\n",
    "        log(\"\\nðŸ“Š Calculando TF...\")\n",
    "        vectorizer_tf = CountVectorizer()\n",
    "        X_tf = vectorizer_tf.fit_transform(clean_corpus)\n",
    "        tf_counts = X_tf.toarray().sum(axis=0)\n",
    "\n",
    "        tf_df = pd.DataFrame({\n",
    "            \"word\": vectorizer_tf.get_feature_names_out(),\n",
    "            \"tf\": tf_counts\n",
    "        }).sort_values(\"tf\", ascending=False)\n",
    "\n",
    "        tf_df.to_csv(\"tf_keywords.csv\", index=False)\n",
    "        log(\"  âœ” Guardado: tf_keywords.csv\")\n",
    "    else:\n",
    "        tf_df = None\n",
    "\n",
    "    # 4. TF-IDF\n",
    "    if var_tfidf.get():\n",
    "        log(\"\\nðŸ“ˆ Calculando TF-IDF...\")\n",
    "        vectorizer_tfidf = TfidfVectorizer(max_df=0.85, min_df=2)\n",
    "        X_tfidf = vectorizer_tfidf.fit_transform(clean_corpus)\n",
    "\n",
    "        tfidf_df = pd.DataFrame({\n",
    "            \"word\": vectorizer_tfidf.get_feature_names_out(),\n",
    "            \"tfidf\": X_tfidf.sum(axis=0).A1\n",
    "        }).sort_values(\"tfidf\", ascending=False)\n",
    "\n",
    "        tfidf_df.to_csv(\"tfidf_keywords.csv\", index=False)\n",
    "        log(\"  âœ” Guardado: tfidf_keywords.csv\")\n",
    "\n",
    "    # 5. N-grams\n",
    "    if var_ngrams.get():\n",
    "        log(\"\\nðŸ”  Extrayendo n-grams...\")\n",
    "        vectorizer_ng = CountVectorizer(ngram_range=(2, 3), min_df=2)\n",
    "        X_ng = vectorizer_ng.fit_transform(clean_corpus)\n",
    "        counts = X_ng.toarray().sum(axis=0)\n",
    "\n",
    "        ng_df = pd.DataFrame({\n",
    "            \"ngram\": vectorizer_ng.get_feature_names_out(),\n",
    "            \"freq\": counts\n",
    "        }).sort_values(\"freq\", ascending=False)\n",
    "\n",
    "        ng_df.to_csv(\"ngrams_keywords.csv\", index=False)\n",
    "        log(\"  âœ” Guardado: ngrams_keywords.csv\")\n",
    "\n",
    "    # 6. Coocurrencias\n",
    "    if var_cooc.get():\n",
    "        if tf_df is None:\n",
    "            messagebox.showwarning(\n",
    "                \"Aviso\",\n",
    "                \"Para coocurrencias es necesario activar TF.\\nOmitiendo.\"\n",
    "            )\n",
    "        else:\n",
    "            log(\"\\nðŸ”— Calculando coocurrencias...\")\n",
    "            cooc = get_cooccurrence_matrix(clean_corpus, tf_df, top_n=50)\n",
    "            cooc.to_csv(\"cooccurrence_matrix.csv\")\n",
    "            log(\"  âœ” Guardado: cooccurrence_matrix.csv\")\n",
    "\n",
    "    log(\"\\nðŸŽ‰ PROCESO COMPLETADO\\n\")\n",
    "    messagebox.showinfo(\"Finalizado\", \"El procesamiento ha terminado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9b726b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Extractor de Palabras Clave desde PDF\")\n",
    "root.geometry(\"700x550\")\n",
    "\n",
    "# Carpeta\n",
    "frame_top = tk.Frame(root)\n",
    "frame_top.pack(pady=10)\n",
    "\n",
    "tk.Label(frame_top, text=\"Carpeta de PDFs:\").grid(row=0, column=0, padx=5)\n",
    "folder_path_var = tk.StringVar()\n",
    "tk.Entry(frame_top, textvariable=folder_path_var, width=50).grid(row=0, column=1)\n",
    "tk.Button(frame_top, text=\"Seleccionar\", command=select_folder).grid(row=0, column=2, padx=5)\n",
    "\n",
    "# Opciones\n",
    "frame_opts = tk.LabelFrame(root, text=\"Opciones a ejecutar\", padx=10, pady=10)\n",
    "frame_opts.pack(pady=10)\n",
    "\n",
    "var_tf = tk.BooleanVar(value=True)\n",
    "var_tfidf = tk.BooleanVar(value=True)\n",
    "var_ngrams = tk.BooleanVar(value=True)\n",
    "var_cooc = tk.BooleanVar(value=True)\n",
    "\n",
    "tk.Checkbutton(frame_opts, text=\"Frecuencias (TF)\", variable=var_tf).pack(anchor=\"w\")\n",
    "tk.Checkbutton(frame_opts, text=\"TF-IDF\", variable=var_tfidf).pack(anchor=\"w\")\n",
    "tk.Checkbutton(frame_opts, text=\"N-grams\", variable=var_ngrams).pack(anchor=\"w\")\n",
    "tk.Checkbutton(frame_opts, text=\"Coocurrencias\", variable=var_cooc).pack(anchor=\"w\")\n",
    "\n",
    "# BotÃ³n ejecutar\n",
    "tk.Button(root, text=\"Ejecutar\", command=run_processing, bg=\"#4CAF50\", fg=\"white\", height=2).pack(pady=10)\n",
    "\n",
    "# Log\n",
    "text_log = tk.Text(root, height=15, width=90)\n",
    "text_log.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Manuel venv)",
   "language": "python",
   "name": "manuel-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
